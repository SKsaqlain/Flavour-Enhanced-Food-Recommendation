{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoCBRS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVOXObXeym74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "326f4c1f-8186-422e-c6aa-bc352d09ffdf"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXWZ59TYyrkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "325e55cf-e387-4c8b-9ddc-0b3c29bfafa5"
      },
      "source": [
        "!pip install prettytable\n",
        "!pip install geocoder\n",
        "!pip install pyowm\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy.sparse import csr_matrix\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "import os.path\n",
        "from prettytable import PrettyTable\n",
        "import geocoder\n",
        "import json\n",
        "import requests\n",
        "import pyowm\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Collecting geocoder\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/6b/13166c909ad2f2d76b929a4227c952630ebaf0d729f6317eb09cbceccbab/geocoder-1.38.1-py2.py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.3MB/s \n",
            "\u001b[?25hCollecting ratelim\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/98/7e6d147fd16a10a5f821db6e25f192265d6ecca3d82957a4fdd592cad49c/ratelim-0.1.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from geocoder) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from geocoder) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from geocoder) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from geocoder) (1.12.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ratelim->geocoder) (4.4.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->geocoder) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->geocoder) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->geocoder) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->geocoder) (3.0.4)\n",
            "Installing collected packages: ratelim, geocoder\n",
            "Successfully installed geocoder-1.38.1 ratelim-0.1.6\n",
            "Collecting pyowm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/2a/83e26bc87763d0d34767ddc5c875608d4a0a0da66e59730a15c55aec6eff/pyowm-2.10.0-py3-none-any.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 2.7MB/s \n",
            "\u001b[?25hCollecting geojson<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from pyowm) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->pyowm) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->pyowm) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->pyowm) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->pyowm) (3.0.4)\n",
            "Installing collected packages: geojson, pyowm\n",
            "Successfully installed geojson-2.5.0 pyowm-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAtJfEtzXzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HealthScoreCalculator():\n",
        "  def __init__(self):\n",
        "    self.weights = {\n",
        "\t'normal' : {'calories' : 1, 'protein' : 1, 'sugar' : 1.1, 'fat' : 1.1, 'sat_fat': 1.7, 'carbs' : 1, 'dietary_fiber' : 1.5, 'sodium' : 1, 'cholesterol' : 1.2, 'vitamin_a' : 1, 'vitamin_c' : 1, 'calcium' : 1, 'iron' : 1}, \n",
        "\t'diabetes' : {'calories' : 1, 'protein' : 1, 'sugar' : 4.25, 'fat' : 1.1, 'sat_fat': 1.7, 'carbs' : 1, 'dietary_fiber' : 3, 'sodium' : 1, 'cholesterol' : 1.2, 'vitamin_a' : 1, 'vitamin_c' : 1, 'calcium' : 1, 'iron' : 1},\n",
        "\t'bp' : {'calories' : 1, 'protein' : 1, 'sugar' : 1.1, 'fat' : 1.1, 'sat_fat': 1.7, 'carbs' : 1, 'dietary_fiber' : 1.5, 'sodium' : 9, 'cholesterol' : 1.2, 'vitamin_a' : 1, 'vitamin_c' : 1, 'calcium' : 1, 'iron' : 1},\n",
        "\t'obesity' : {'calories' : 7, 'protein' : 1, 'sugar' : 1.1, 'fat' : 1.1, 'sat_fat': 1.7, 'carbs' : 1, 'dietary_fiber' : 1.5, 'sodium' : 1, 'cholesterol' : 1.2, 'vitamin_a' : 1, 'vitamin_c' : 1, 'calcium' : 1, 'iron' : 1}\n",
        "\t}\n",
        "    self.predictedScores=None\n",
        "\n",
        "  def convToComStand(self,value):\n",
        "    if value.endswith('mg'):\n",
        "      value = value[:value.index('mg')]\n",
        "      value = value.replace(\",\", \"\")\n",
        "      value = value.replace(\" \", \"\")\n",
        "      value = float(value) / 1000\n",
        "    elif value.endswith('g'):\n",
        "      value = value[:value.index('g')]\n",
        "      value = value.replace(\",\", \"\")\n",
        "      value = value.replace(\" \", \"\")\n",
        "      value = float(value)\n",
        "    elif value.endswith('kcal'):\n",
        "      value = value[:value.index('kcal')]\n",
        "      value = value.replace(\",\", \"\")\n",
        "      value = value.replace(\" \", \"\")\n",
        "      value = float(value)\n",
        "\n",
        "    elif value.endswith('IU'):\n",
        "      value = value[:value.index('IU')]\n",
        "      value = value.replace(\",\", \"\")\n",
        "      value = value.replace(\" \", \"\")\n",
        "      value = (float(value) / 3.3) / 1000000\n",
        "    \n",
        "    return value\n",
        "\n",
        "  def elixir(self,allowed, weights, bmratio, type = 'normal'):\n",
        "    weights = weights[type]\n",
        "    database = json.load(open(\"/content/gdrive/My Drive/Final Year Project/DeepConn/Deep_Learning_Recommender_System/database.json\", 'rb'))\n",
        "    x = {}\n",
        "    for dish in database:\n",
        "      dishId = dish['dish_id']\n",
        "      nutrients = dish['nutrients']\n",
        "      if all(nutrients.values()):\n",
        "        for nutrient in nutrients:\n",
        "          value = nutrients[nutrient]\n",
        "          if value:\n",
        "            #converting to  common standard\n",
        "            value = self.convToComStand(value[0])\n",
        "            nutrients[nutrient] = value\n",
        "        #################################\n",
        "        recbn = ['protein', 'dietary_fiber']\n",
        "        recbase = 0\n",
        "        for i in recbn:\n",
        "          recbase += weights[i] * nutrients[i] / allowed[i]\n",
        "\n",
        "        part1 = 0\n",
        "        part2 = 0\n",
        "        if nutrients['carbs']:\n",
        "          part1 = weights['dietary_fiber'] * nutrients['dietary_fiber'] / nutrients['carbs']\n",
        "          part2 =  0.1 * (nutrients['carbs'] - nutrients['dietary_fiber'] - nutrients['sugar']) / nutrients['carbs']\n",
        "\n",
        "        recbase = recbase + part1 + part2\n",
        "        #################################\n",
        "        restbn = ['carbs', 'cholesterol', 'sodium', 'sat_fat', 'fat', 'sugar']\n",
        "        restbase = 0\n",
        "        for i in restbn:\n",
        "          restbase += weights[i] * nutrients[i] / allowed[i]\n",
        "\n",
        "        part1 = 0\n",
        "        part2 = 0\n",
        "        if nutrients['sugar'] and nutrients['carbs']:\n",
        "          part1 = weights['carbs'] * nutrients['sugar'] / nutrients['carbs']\n",
        "\n",
        "        if nutrients['sat_fat'] and nutrients['fat']:\n",
        "          part2 = weights['sat_fat'] * nutrients['sat_fat'] / nutrients['fat']\n",
        "        \n",
        "        restbase = part1 + part2\t\t\t\n",
        "        #################################\n",
        "        recan = ['vitamin_a', 'vitamin_c', 'calcium', 'iron']\n",
        "        recadd = 0\n",
        "        for i in recan:\n",
        "          recadd += weights[i] * nutrients[i] / allowed[i]\n",
        "        #################################\n",
        "\n",
        "        mult = bmratio\n",
        "        div = ((1 + mult) * restbase)\n",
        "        if div:\n",
        "          score = recbase + (mult * recadd) / div\n",
        "        else:\n",
        "          score = 0\n",
        "\n",
        "        x[dishId] = score\n",
        "    self.predictedScores=x\n",
        "    return x\n",
        "\n",
        "  def get_lat_long(self,ip_addr =\"me\"):\n",
        "    g = geocoder.ip(ip_addr)\n",
        "    return g.latlng\n",
        "\n",
        "  def get_temp(self,lat, lon):\n",
        "    owm = pyowm.OWM('9bb248641cc71b7ab1b7040317ec6ac9')\n",
        "    observation_list = owm.weather_around_coords(lat, lon)\n",
        "    tot_temp = 0\n",
        "    if observation_list != None:\n",
        "      for i in observation_list:\n",
        "        w = i.get_weather()\n",
        "        temp = w.get_temperature('fahrenheit')['temp']\n",
        "        tot_temp += temp\n",
        "\n",
        "      temp = tot_temp / len(observation_list)\n",
        "\n",
        "      return temp\n",
        "\n",
        "  def elevation(self,lat, lng):\n",
        "    loc = str(lat) + ',' + str(lng)\n",
        "    apikey = \"AIzaSyAd0cewCPnbRF_0082PULMybInWNhWgDjA\"\n",
        "    base_url = \"https://maps.googleapis.com/maps/api/elevation/json\"\n",
        "    params = dict()\n",
        "    params[\"locations\"] = str(loc)\n",
        "    params[\"key\"] = apikey\n",
        "    r = requests.get(base_url, params=params)\n",
        "    results = json.loads(r.text).get('results')\n",
        "    return 847\n",
        "    return results[0]['elevation']\n",
        "\n",
        "  def adaptive_daily_value(self,weight, height, gender, age, steps, height_travelled, bmratio):\n",
        "    latlong = self.get_lat_long()\n",
        "    temp = self.get_temp(*latlong)\n",
        "    altitude =self.elevation(*latlong)\n",
        "\n",
        "    allowed = {'calories' : 2079.35, 'protein' : 50, 'fat' : 70, 'sat_fat' : 24, 'carbs' : 310, 'sugar' : 30, 'dietary_fiber' : 30, 'sodium' : 2.3, 'cholesterol' : 300, 'vitamin_a' : 0.0008, 'vitamin_c' : 0.08, 'iron' : 0.0087, 'calcium' : 1}\n",
        "    \n",
        "    work = 9.8 * weight * height_travelled * 0.000239006 + weight * steps / 6000\n",
        "    \n",
        "    bmr = weight * 10 + 6.25 * height - 5 * age\n",
        "    if gender == 'm':\n",
        "      bmr += 5\n",
        "    else:\n",
        "      bmr -= 161\n",
        "\n",
        "    daily_cal = round(bmratio * bmr + work, 2)\n",
        "    if temp != None:\n",
        "      daily_cal = daily_cal * (1 + (85 - temp) / 800)\n",
        "\n",
        "      for i in allowed:\n",
        "        allowed[i] = allowed[i] * daily_cal / 2079.35\n",
        "\n",
        "    na_multi = 1 + 0.015 * (((temp - 32) * 0.56) - 23)\n",
        "    allowed['sodium'] = na_multi * allowed['sodium'] + (altitude / 1000) ** 2.5\n",
        "\n",
        "    return allowed\n",
        "  def calculateScore(self,height,weight,age,gender,condition,bmratio,steps, floors):\n",
        "    floors = floors * 10 * 3.28084\n",
        "    allowed = self.adaptive_daily_value(weight, height, gender, age, steps, floors, bmratio)\n",
        "    score = self.elixir(allowed, self.weights, bmratio, type = condition)\n",
        "    self.predictedScores=score\n",
        "    return score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQXuWsLO0Y7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentBasedRecommendationSystem(HealthScoreCalculator):\n",
        "  def __init__(self):\n",
        "    HealthScoreCalculator.__init__(self)\n",
        "    self.predictionedRatings=pd.DataFrame()\n",
        "    self.tfidf=None\n",
        "  def append_to_data(self,data, profile, predict_on):\n",
        "    profile = json.loads(profile)\n",
        "    dish_ids = list(map(int, profile.keys()))\n",
        "    ratings = list(map(int, profile.values()))\n",
        "\n",
        "    d = pd.DataFrame(columns = ['dishId', 'userId', 'rating'])\n",
        "    d['dishId'] = dish_ids\n",
        "    d['rating'] = ratings\n",
        "    d['userId'] = predict_on\n",
        "\n",
        "    data = data.append(d)\n",
        "    return data\n",
        "  def tokenize_string(self,my_string):\n",
        "    #return re.findall('[\\w\\-]+', my_string.lower())\n",
        "    return [ele.strip() for ele in my_string.split(\"|\")]\n",
        "  \n",
        "  def tokenize(self,db):\n",
        "    \"\"\"\n",
        "    The meta tags associated with each dish is broken down (tokenized) as a list of tags\n",
        "    Eg: egg|flour|ghee|paratha will be tokenized as [egg, flour, ghee, paratha]\n",
        "    \"\"\"\n",
        "    tokenlist=[]\n",
        "    for index,row in db.iterrows():\n",
        "        tokenlist.append(self.tokenize_string(row.tags))\n",
        "    db['tokens']=tokenlist\n",
        "    return db\n",
        "\n",
        "  def featurize(self,db, include_flavours):\n",
        "    \n",
        "    def tf(word, doc):\n",
        "        return doc.count(word) / Counter(doc).most_common()[0][1]\n",
        "\n",
        "    def df(word, doclist):\n",
        "        return sum(1 for d in doclist if word in d)\n",
        "\n",
        "    def tfidf(word, doc, dfdict, N):\n",
        "        return tf(word, doc) * (math.log10((N / dfdict[word])))\n",
        "\n",
        "    def getcsrmatrix(tokens,dfdict,N,vocab, dish_flavours, max_vocab):\n",
        "        matrixRow_list = []\n",
        "        if include_flavours:\n",
        "            matrixRow_list = np.zeros((1,len(vocab) + len(dish_flavours) - 1),dtype='float')\n",
        "        else:\n",
        "            matrixRow_list = np.zeros((1,len(vocab)),dtype='float')\n",
        "        for t in tokens:\n",
        "            if t in vocab:\n",
        "                matrixRow_list[0][vocab[t]] = tfidf(t,tokens,dfdict,N)\n",
        "\n",
        "        if include_flavours:\n",
        "            matrixRow_list[0][max_vocab] = dish_flavours['bitter']\n",
        "            matrixRow_list[0][max_vocab+1] = dish_flavours['rich']\n",
        "            matrixRow_list[0][max_vocab + 2] = dish_flavours['salt']\n",
        "            matrixRow_list[0][max_vocab + 3] = dish_flavours['spicy']\n",
        "            matrixRow_list[0][max_vocab + 4] = dish_flavours['sweet']\n",
        "            matrixRow_list[0][max_vocab + 5] = dish_flavours['umami']\n",
        "\n",
        "        return csr_matrix(matrixRow_list)\n",
        "\n",
        "    flavour = pd.read_csv('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tastes.csv', names = ['dishId', 'bitter', 'rich', 'salt', 'spicy', 'sweet', 'umami'])\n",
        "    print(\"flavour shape= \",flavour.shape)\n",
        "    print(flavour.head())\n",
        "    N=len(db)\n",
        "    \n",
        "\n",
        "    doclist = db['tokens'].tolist()\n",
        "    # print(\"tokens = \",doclist[:2])\n",
        "    print(\"doclist type=\",type(doclist))\n",
        "    vocab = { i:x for x,i in enumerate(sorted(list(set(i for s in doclist for i in s))))}\n",
        "    #print(vocab)\n",
        "    max_vocab = max(vocab.values()) + 1\n",
        "    print(\"max vocab= \",max_vocab)\n",
        "    \n",
        "    dfdict = {}\n",
        "    for v in vocab.items():\n",
        "        dfdict[v[0]] = df(v[0],doclist)\n",
        "\n",
        "    csrlist = []\n",
        "    for index, row in db.iterrows():\n",
        "        dish_flavours = flavour[flavour.dishId == row['dishId']].to_dict(orient = 'record')[0]\n",
        "        csrlist.append(getcsrmatrix(row['tokens'],dfdict,N,vocab, dish_flavours, max_vocab)) # row['dishId'] and df with flavour scores\n",
        "    \n",
        "    db['features'] =  csrlist\n",
        "    print(\"after including ifidf features\")\n",
        "    print(db.head())\n",
        "    return (db,vocab)\n",
        "  def my_train_test_split(self,ratings):\n",
        "    \n",
        "    # train_set, test_set = train_test_split(ratings, test_size = 0.20, random_state = 42,stratify=ratings[\"rating\"])\n",
        "    train_set, test_set = train_test_split(ratings, test_size = 0.20, random_state = 42)\n",
        "    return train_set, test_set\n",
        "  def cosine_sim(self,a, b, include_flavours):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    v1 = a.toarray()[0]\n",
        "    v2  = b.toarray()[0]\n",
        "    def cos_sim(v1, v2):\n",
        "        x = (math.sqrt(sum([i*i for i in v1]))*math.sqrt(sum([i*i for i in v2])))\n",
        "        if x:\n",
        "            return sum(i[0] * i[1] for i in zip(v1, v2)) / x\n",
        "        else:\n",
        "            return 0\n",
        "    # s1 = cos_sim(v1, v2)\n",
        "    # return s1\n",
        "    \n",
        "    s1 = cos_sim(v1[:-6], v2[:-6])\n",
        "    if include_flavours:\n",
        "        s2 = cos_sim(v1[-6:], v2[-6:])\n",
        "        return s1 * 0.5 + s2 * 0.5\n",
        "    else:\n",
        "        return s1\n",
        "    \n",
        "    #s1=cos_sim(v1,v2)\n",
        "    return s1\n",
        "\n",
        "  def make_predictions(self,db, ratings_train, ratings_test, include_flavours):\n",
        "    result = []\n",
        "    x = 0\n",
        "    for index,row in ratings_test.iterrows():\n",
        "        # mlist contains dishIds rated by the user in the train set\n",
        "        mlist = list(ratings_train.loc[ratings_train['userId'] == row['userId']]['dishId'])\n",
        "        #print(\"dishes rated by user \",row[\"userId\"],\" \",mlist)\n",
        "        # csr list contains tfidf scores of tags for dishes rated by the user\n",
        "        csrlist = list(db.loc[db['dishId'].isin(mlist)]['features'])\n",
        "        #print(\"csrlist \",csrlist)\n",
        "        # mrlist contains scores of dishes rated by the user (dishes in mlist)\n",
        "        mrlist = list(ratings_train.loc[ratings_train['userId'] == row['userId']]['rating'])\n",
        "        #print(\"mrlist \",mrlist)\n",
        "        # computing similarity between dishes user rated and the current dish in the test set\n",
        "\n",
        "        # l=[0]*len(db[\"dishId\"])\n",
        "        # for i,ele in enumerate(db[\"dishId\"]):\n",
        "        #     if(int(ele)==int(row[\"dishId\"])):\n",
        "        #         l[i]=1\n",
        "        #print(row[\"dishId\"],type(row[\"dishId\"]),type(db[\"dishId\"]))\n",
        "        sim = [self.cosine_sim(c,db.loc[db['dishId'] ==int(row['dishId'])]['features'].values[0], include_flavours) for c in csrlist]\n",
        "        # computing similarity times the rating for known dish\n",
        "        wan = sum([ v*mrlist[i] for i,v in enumerate(sim) if v>0])\n",
        "        wadlist = [i for i in sim if i>0]\n",
        "        ## check for sum(wadlist) > 1\n",
        "        if len(wadlist)>0 and sum(wadlist) >= 1:\n",
        "            result.append(wan/sum(wadlist))\n",
        "            x = x + 1\n",
        "        else:\n",
        "            #print(\"here\")\n",
        "            result.append(np.mean(mrlist)) # if dish did not match with anything approx as average of users rating\n",
        "    return np.array(result)\n",
        "\n",
        "  def main(self,data, db, predict_on, include_flavours):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    total_dishes = db.shape[0]\n",
        "    print(\"In main total dishes= \",total_dishes)\n",
        "    \n",
        "    db = self.tokenize(db)\n",
        "    print(\"after tokenizing db= \",db.shape)\n",
        "    print(db.head())\n",
        "    \n",
        "    db, vocab = self.featurize(db, include_flavours)\n",
        "    def dummy_fun(doc):\n",
        "      return doc\n",
        "    if(self.tfidf==None):\n",
        "      self.tfidf = TfidfVectorizer(\n",
        "          analyzer='word',\n",
        "          tokenizer=dummy_fun,\n",
        "          preprocessor=dummy_fun,\n",
        "          token_pattern=None,smooth_idf=True)\n",
        "      self.tfidf.fit(db[\"tokens\"])\n",
        "    self.tfidf_features=self.tfidf.transform(db[\"tokens\"])\n",
        "    print(\"after tfidf \",self.tfidf_features.shape)\n",
        "    db[\"features\"]=list(self.tfidf_features)\n",
        "\n",
        "    ratings_train, ratings_test = self.my_train_test_split(data)\n",
        "    \n",
        "    print(\"trainig shape= \",ratings_train.shape)\n",
        "    print(\"testing shape= \",ratings_test.shape)\n",
        "\n",
        "    predictions = self.make_predictions(db, ratings_train, ratings_test, include_flavours)\n",
        "\n",
        "    predicted_test_error = mean_squared_error(ratings_test.rating, predictions) ** 0.5\n",
        "\n",
        "    def predict_on_user(predict_on):\n",
        "        ratings_test = pd.DataFrame(columns = ['userId', 'dishId'])\n",
        "        ratings_test['userId'] = [predict_on] * total_dishes\n",
        "        ratings_test.dishId = range(1, total_dishes + 1)\n",
        "           \n",
        "        predictions_uid = self.make_predictions(db, ratings_train, ratings_test, include_flavours)\n",
        "\n",
        "        predictions_uid = list(enumerate(predictions_uid))\n",
        "\n",
        "        predictions_uid = sorted(predictions_uid, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "        predictions_uid = list(map(lambda x: (x[0] + 1, x[1]), predictions_uid))\n",
        "\n",
        "        return predictions_uid\n",
        "    \n",
        "    print(\"predicted_on\",predict_on)\n",
        "    return (predicted_test_error, predict_on_user(predict_on = predict_on))\n",
        "\n",
        "  def start(self,profile = None, type = 'meta', predict_on = 100, flavours = False, retrain = False):\n",
        "    import time\n",
        "    time_start = time.time()\n",
        "    data = pd.read_csv('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/review.csv')\n",
        "    \n",
        "    data=data[data[\"dishId\"]!=1381]\n",
        "    print(\"before elimination =\",data.shape)\n",
        "    print(data.head())\n",
        "    data = data[data['userId'].isin(data['userId'].value_counts()[data['userId'].value_counts() >= 5].index)]\n",
        "    print(\"after elimination =\",data.shape)\n",
        "\n",
        "    if not retrain:\n",
        "        if flavours:\n",
        "            final_scores = pickle.load(open(\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_flavour_scores.pickle\", \"rb\" ))\n",
        "            predictions = final_scores[predict_on]\n",
        "\n",
        "        else:\n",
        "            final_scores = pickle.load(open(\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_scores.pickle\", \"rb\" ))\n",
        "            predictions = final_scores[predict_on]\n",
        "\n",
        "        predicted_test_error = None\n",
        "\n",
        "    else:\n",
        "        if profile:\n",
        "            data = self.append_to_data(data, profile, predict_on)\n",
        "\n",
        "        if type == 'all':\n",
        "            db = pd.read_csv('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/meta_cuisine.csv')\n",
        "        elif type == 'meta':\n",
        "            db = pd.read_csv('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/newDatabase.csv', names = ['dishId', 'tags'])\n",
        "            #db = pd.read_csv('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/database.csv', names = ['dishId', 'tags'])\n",
        "            # newDb=pd.concat([db,db2],axis=1,sort=False)\n",
        "            # newDb.head()\n",
        "        #db=db.dropna()\n",
        "        print(\"food db size= \",db.shape)\n",
        "        print(db.head())\n",
        "        \n",
        "        dishes = pd.read_csv('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/id_name_mapping.csv', names = ['dishId', 'dish_name'])\n",
        "        print(\"dishes size= \",dishes.shape)\n",
        "        print(dishes.head())\n",
        "        \n",
        "        predicted_test_error, predictions = self.main(data, db, predict_on = predict_on, include_flavours = flavours)\n",
        "        \n",
        "        predictions = pd.DataFrame(predictions, columns = ['dishId', 'rating'])\n",
        "        predictions = predictions.merge(dishes, on = 'dishId', how = 'left')\n",
        "        predictions.columns = ['dishId', 'rating', 'dishName']\n",
        "\n",
        "        if flavours:\n",
        "            if os.path.exists(\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_flavour_scores.pickle\"):\n",
        "                final_scores = pickle.load(open(\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_flavour_scores.pickle\", \"rb\" ))\n",
        "                final_scores[predict_on] = predictions\n",
        "            else:\n",
        "                final_scores = {}\n",
        "                final_scores[predict_on] = predictions\n",
        "\n",
        "            pickle.dump(final_scores, open('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_flavour_scores.pickle', 'wb'))\n",
        "\n",
        "        else:\n",
        "            if os.path.exists(\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_scores.pickle\"):\n",
        "                final_scores = pickle.load(open(\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_scores.pickle\", \"rb\" ))\n",
        "                final_scores[predict_on] = predictions\n",
        "\n",
        "            else:\n",
        "                final_scores = {}\n",
        "                final_scores[predict_on] = predictions\n",
        "\n",
        "            pickle.dump(final_scores, open('/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/tfidf_final_scores.pickle', 'wb'))\n",
        "\n",
        "    data = data[data.userId == predict_on]\n",
        "    data[\"dishId\"]=data[\"dishId\"].astype(np.int64)\n",
        "    original_rating = data.merge(predictions, how = 'left', on = 'dishId')\n",
        "    original_rating.columns = ['dishId', 'userId', 'rating', 'reformed', 'dishName']\n",
        "    \n",
        "    time_end = time.time()\n",
        "    self.predictedRatings=predictions\n",
        "    answer = {\"user\" : predict_on, \"predicted_test_error\": predicted_test_error, \"time\" : round(time_end - time_start, 2), \"predicted_rating\" : predictions, \"original_rating\" : original_rating}\n",
        "    return answer\n",
        "\n",
        "  def recommendDishes(self,profile=None):\n",
        "    if(self.predictedRatings.shape[0]==0):\n",
        "      print(\"Trinaing the model\")\n",
        "      x=self.start(retrain=True,predict_on=100,profile=profile)\n",
        "    elif(self.predictedRatings.shape[0]!=0 and profile==None):\n",
        "      x=self.start(retrain=False,predict_on=100,profile=profile)\n",
        "    else:\n",
        "      x=self.start(retrain=True,profile=profile,predict_on=100)\n",
        "\n",
        "    if(self.predictedScores==None):\n",
        "      raise Exception(\"HealthScores are not calculated\")\n",
        "    scores=pd.DataFrame(list(self.predictedScores.items()),columns=[\"dishId\",\"score\"])\n",
        "    ratings=self.predictedRatings\n",
        "    df=pd.merge(ratings,scores,on=\"dishId\")\n",
        "    sortedDf=df.sort_values(by='rating',ascending=False)[:10]\n",
        "    sortedDf=sortedDf.sort_values(by='score',ascending=False)\n",
        "    return sortedDf  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn7Nw8-aJJA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "outputId": "f93ea1b4-4d09-4bca-8366-3902d8f33d4b"
      },
      "source": [
        "model=ContentBasedRecommendationSystem()\n",
        "y=model.start(retrain=True,predict_on=100,flavours=False)\n",
        "healthScores=model.calculateScore(5.6,60,25,'m','normal',3000,10,22.1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before elimination = (30165, 3)\n",
            "   dishId  userId  rating\n",
            "0     291       0       5\n",
            "1     291       1       4\n",
            "2     291       2       4\n",
            "3     291       3       5\n",
            "4     291       4       4\n",
            "after elimination = (3975, 3)\n",
            "food db size=  (1380, 2)\n",
            "   dishId                                               tags\n",
            "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...\n",
            "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...\n",
            "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...\n",
            "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...\n",
            "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...\n",
            "dishes size=  (1381, 2)\n",
            "   dishId                  dish_name\n",
            "0       1   curried green bean salad\n",
            "1       2                 keema aloo\n",
            "2       3                    paratha\n",
            "3       4    black chana with potato\n",
            "4       5  tomato cucumber kachumbar\n",
            "In main total dishes=  1380\n",
            "after tokenizing db=  (1380, 3)\n",
            "   dishId  ...                                             tokens\n",
            "0       1  ...  [Ethyl Lactate, 3,4-Dihydroxybenzaldehyde, DL-...\n",
            "1       2  ...  [AC1LDI49, 56424-87-4, 3,4-Dihydroxybenzaldehy...\n",
            "2       3  ...  [3-Methyl-1-butanol, Thymol, 2-Nonanone, Pyrro...\n",
            "3       4  ...  [AC1LDI49, 56424-87-4, 2-Hexenyl propanoate, 3...\n",
            "4       5  ...  [3,4-Dihydroxybenzaldehyde, DL-Liquiritigenin,...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "flavour shape=  (1381, 7)\n",
            "   dishId  bitter  rich   salt  spicy  sweet  umami\n",
            "0       1   0.961  0.71  4.567   5.20   3.84      1\n",
            "1       2   3.876  4.50  0.240   4.56   0.38      8\n",
            "2       3   0.000  2.00  2.725  10.00   0.14      0\n",
            "3       4   4.672  0.87  0.294   3.37   1.91      6\n",
            "4       5   0.813  0.00  6.173   8.02   3.23      6\n",
            "doclist type= <class 'list'>\n",
            "max vocab=  1405\n",
            "after including ifidf features\n",
            "   dishId  ...                                           features\n",
            "0       1  ...    (0, 0)\\t0.5555478620337058\\n  (0, 4)\\t0.0249...\n",
            "1       2  ...    (0, 0)\\t0.5555478620337058\\n  (0, 3)\\t0.6122...\n",
            "2       3  ...    (0, 106)\\t0.04436104407808569\\n  (0, 107)\\t0...\n",
            "3       4  ...    (0, 0)\\t0.5555478620337058\\n  (0, 4)\\t0.0249...\n",
            "4       5  ...    (0, 4)\\t0.02493467068865182\\n  (0, 8)\\t0.024...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "after tfidf  (1380, 1405)\n",
            "trainig shape=  (3180, 3)\n",
            "testing shape=  (795, 3)\n",
            "predicted_on 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iafChvrUBwDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "a5c1bca6-35b1-4a44-eb06-86a73bfba743"
      },
      "source": [
        "y"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_rating':    dishId  userId  rating  reformed             dishName\n",
              " 0     276     100       5  4.328285          mango lassi\n",
              " 1      15     100       5  4.318062        chicken curry\n",
              " 2     176     100       4  4.325148        nariyal burfi\n",
              " 3      16     100       5  4.399579      chicken makhani\n",
              " 4     150     100       5  4.505872     strawberry lassi\n",
              " 5      13     100       5  4.441553     vegetarian korma\n",
              " 6      31     100       5  4.475859          mango lassi\n",
              " 7     206     100       3  4.425210                pulao\n",
              " 8      92     100       3  4.378520  curried cauliflower\n",
              " 9     245     100       4  4.475859          mango lassi,\n",
              " 'predicted_rating':       dishId    rating                  dishName\n",
              " 0        584  4.558417          strawberry lassi\n",
              " 1       1373  4.550255                   waffles\n",
              " 2        884  4.548975         chocolate brownie\n",
              " 3        885  4.548975  fudgy chocolate brownies\n",
              " 4          3  4.543339                   paratha\n",
              " ...      ...       ...                       ...\n",
              " 1375     983  4.269363         fish steaks dijon\n",
              " 1376    1376  4.268147          watermelon juice\n",
              " 1377    1045  4.222747              lemon pickle\n",
              " 1378     981  4.215693       grilled fish steaks\n",
              " 1379    1377  4.037594          watermelon juice\n",
              " \n",
              " [1380 rows x 3 columns],\n",
              " 'predicted_test_error': 1.0598732671609836,\n",
              " 'time': 110.35,\n",
              " 'user': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANqTM9lCit2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "3ae70e49-9b2d-4d82-db5a-242a82b53081"
      },
      "source": [
        "model.recommendDishes()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before elimination = (30165, 3)\n",
            "   dishId  userId  rating\n",
            "0     291       0       5\n",
            "1     291       1       4\n",
            "2     291       2       4\n",
            "3     291       3       5\n",
            "4     291       4       4\n",
            "after elimination = (3975, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dishId</th>\n",
              "      <th>rating</th>\n",
              "      <th>dishName</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4.543339</td>\n",
              "      <td>paratha</td>\n",
              "      <td>0.239879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>892</td>\n",
              "      <td>4.523092</td>\n",
              "      <td>chocolate milkshake</td>\n",
              "      <td>0.121095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1373</td>\n",
              "      <td>4.550255</td>\n",
              "      <td>waffles</td>\n",
              "      <td>0.101368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>884</td>\n",
              "      <td>4.548975</td>\n",
              "      <td>chocolate brownie</td>\n",
              "      <td>0.097573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>891</td>\n",
              "      <td>4.525108</td>\n",
              "      <td>chocolate cheesecake</td>\n",
              "      <td>0.072577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>889</td>\n",
              "      <td>4.524212</td>\n",
              "      <td>chocolate cheesecake</td>\n",
              "      <td>0.072577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>890</td>\n",
              "      <td>4.525463</td>\n",
              "      <td>white chocolate cheesecake</td>\n",
              "      <td>0.054342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>885</td>\n",
              "      <td>4.548975</td>\n",
              "      <td>fudgy chocolate brownies</td>\n",
              "      <td>0.036319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>584</td>\n",
              "      <td>4.558417</td>\n",
              "      <td>strawberry lassi</td>\n",
              "      <td>0.019902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1060</td>\n",
              "      <td>4.542033</td>\n",
              "      <td>kaju katli</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dishId    rating                    dishName     score\n",
              "4       3  4.543339                     paratha  0.239879\n",
              "9     892  4.523092         chocolate milkshake  0.121095\n",
              "1    1373  4.550255                     waffles  0.101368\n",
              "2     884  4.548975           chocolate brownie  0.097573\n",
              "7     891  4.525108        chocolate cheesecake  0.072577\n",
              "8     889  4.524212        chocolate cheesecake  0.072577\n",
              "6     890  4.525463  white chocolate cheesecake  0.054342\n",
              "3     885  4.548975    fudgy chocolate brownies  0.036319\n",
              "0     584  4.558417            strawberry lassi  0.019902\n",
              "5    1060  4.542033                  kaju katli  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3sV8w9mkiRz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f4a161c-3067-4620-e24f-4a8b35fd6e32"
      },
      "source": [
        "x=json.dumps({\"3\":\"3\",\"892\":\"4\",\"1373\":\"3\",\"884\":\"4\",\"891\":\"5\",\"890\":\"5\",\"584\":\"3\"})\n",
        "model.recommendDishes(profile=x)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before elimination = (30165, 3)\n",
            "   dishId  userId  rating\n",
            "0     291       0       5\n",
            "1     291       1       4\n",
            "2     291       2       4\n",
            "3     291       3       5\n",
            "4     291       4       4\n",
            "after elimination = (3975, 3)\n",
            "food db size=  (1380, 2)\n",
            "   dishId                                               tags\n",
            "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...\n",
            "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...\n",
            "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...\n",
            "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...\n",
            "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...\n",
            "dishes size=  (1381, 2)\n",
            "   dishId                  dish_name\n",
            "0       1   curried green bean salad\n",
            "1       2                 keema aloo\n",
            "2       3                    paratha\n",
            "3       4    black chana with potato\n",
            "4       5  tomato cucumber kachumbar\n",
            "In main total dishes=  1380\n",
            "after tokenizing db=  (1380, 3)\n",
            "   dishId  ...                                             tokens\n",
            "0       1  ...  [Ethyl Lactate, 3,4-Dihydroxybenzaldehyde, DL-...\n",
            "1       2  ...  [AC1LDI49, 56424-87-4, 3,4-Dihydroxybenzaldehy...\n",
            "2       3  ...  [3-Methyl-1-butanol, Thymol, 2-Nonanone, Pyrro...\n",
            "3       4  ...  [AC1LDI49, 56424-87-4, 2-Hexenyl propanoate, 3...\n",
            "4       5  ...  [3,4-Dihydroxybenzaldehyde, DL-Liquiritigenin,...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "flavour shape=  (1381, 7)\n",
            "   dishId  bitter  rich   salt  spicy  sweet  umami\n",
            "0       1   0.961  0.71  4.567   5.20   3.84      1\n",
            "1       2   3.876  4.50  0.240   4.56   0.38      8\n",
            "2       3   0.000  2.00  2.725  10.00   0.14      0\n",
            "3       4   4.672  0.87  0.294   3.37   1.91      6\n",
            "4       5   0.813  0.00  6.173   8.02   3.23      6\n",
            "doclist type= <class 'list'>\n",
            "max vocab=  1405\n",
            "after including ifidf features\n",
            "   dishId  ...                                           features\n",
            "0       1  ...    (0, 0)\\t0.5555478620337058\\n  (0, 4)\\t0.0249...\n",
            "1       2  ...    (0, 0)\\t0.5555478620337058\\n  (0, 3)\\t0.6122...\n",
            "2       3  ...    (0, 106)\\t0.04436104407808569\\n  (0, 107)\\t0...\n",
            "3       4  ...    (0, 0)\\t0.5555478620337058\\n  (0, 4)\\t0.0249...\n",
            "4       5  ...    (0, 4)\\t0.02493467068865182\\n  (0, 8)\\t0.024...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "after tfidf  (1380, 1405)\n",
            "trainig shape=  (3185, 3)\n",
            "testing shape=  (797, 3)\n",
            "predicted_on 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dishId</th>\n",
              "      <th>rating</th>\n",
              "      <th>dishName</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>573</td>\n",
              "      <td>4.325460</td>\n",
              "      <td>goat curry</td>\n",
              "      <td>0.476937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13</td>\n",
              "      <td>4.332847</td>\n",
              "      <td>vegetarian korma</td>\n",
              "      <td>0.428682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1019</td>\n",
              "      <td>4.327706</td>\n",
              "      <td>mughlai aloo matar gobi</td>\n",
              "      <td>0.404012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>434</td>\n",
              "      <td>4.327095</td>\n",
              "      <td>zucchini onion pepper latkes</td>\n",
              "      <td>0.371386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>247</td>\n",
              "      <td>4.325327</td>\n",
              "      <td>quinoa biryani</td>\n",
              "      <td>0.353830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>518</td>\n",
              "      <td>4.331699</td>\n",
              "      <td>vegetable pulao</td>\n",
              "      <td>0.276715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>194</td>\n",
              "      <td>4.337667</td>\n",
              "      <td>spiced rice</td>\n",
              "      <td>0.246232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>158</td>\n",
              "      <td>4.327846</td>\n",
              "      <td>vegetable bhaji</td>\n",
              "      <td>0.223505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>551</td>\n",
              "      <td>4.323640</td>\n",
              "      <td>goat biryani</td>\n",
              "      <td>0.172537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1106</td>\n",
              "      <td>4.328386</td>\n",
              "      <td>methi malai matar</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dishId    rating                      dishName     score\n",
              "7     573  4.325460                    goat curry  0.476937\n",
              "1      13  4.332847              vegetarian korma  0.428682\n",
              "5    1019  4.327706       mughlai aloo matar gobi  0.404012\n",
              "6     434  4.327095  zucchini onion pepper latkes  0.371386\n",
              "8     247  4.325327                quinoa biryani  0.353830\n",
              "2     518  4.331699               vegetable pulao  0.276715\n",
              "0     194  4.337667                   spiced rice  0.246232\n",
              "4     158  4.327846               vegetable bhaji  0.223505\n",
              "9     551  4.323640                  goat biryani  0.172537\n",
              "3    1106  4.328386             methi malai matar  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OLOlDiwrEUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}