{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoFlavourCBRS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVr6GcZT45SG",
        "colab_type": "code",
        "outputId": "2713d764-58cc-4080-dd49-c7f2f4ed9a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNd6JU09zboq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy.sparse import csr_matrix\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "import os.path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqKUehXEy7DJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "my_path = ''\n",
        "\n",
        "def append_to_data(data, profile, predict_on):\n",
        "    profile = json.loads(profile)\n",
        "    dish_ids = list(map(int, profile.keys()))\n",
        "    ratings = list(map(int, profile.values()))\n",
        "\n",
        "    d = pd.DataFrame(columns = ['dishId', 'userId', 'rating'])\n",
        "    d['dishId'] = dish_ids\n",
        "    d['rating'] = ratings\n",
        "    d['userId'] = predict_on\n",
        "\n",
        "    data = data.append(d)\n",
        "    return data\n",
        "\n",
        "def tokenize_string(my_string):\n",
        "    return re.findall('[\\w\\-]+', my_string.lower())\n",
        "\n",
        "\n",
        "def tokenize(db):\n",
        "    \"\"\"\n",
        "    The meta tags associated with each dish is broken down (tokenized) as a list of tags\n",
        "    Eg: egg|flour|ghee|paratha will be tokenized as [egg, flour, ghee, paratha]\n",
        "    \"\"\"\n",
        "    tokenlist=[]\n",
        "    for index,row in db.iterrows():\n",
        "        tokenlist.append(tokenize_string(row.tags))\n",
        "    db['tokens']=tokenlist\n",
        "    return db\n",
        "\n",
        "def featurize(db, include_flavours):\n",
        "    \"\"\"\n",
        "    Each row will contain a csr_matrix of shape (1, num_features).\n",
        "    Each entry in this matrix will contain the tf-idf value of the term\n",
        "    Formula : tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
        "    where:\n",
        "    i is a term\n",
        "    d is a document \n",
        "    tf(i, d) is the frequency of term i in document d\n",
        "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
        "    N is the number of documents\n",
        "    \"\"\"\n",
        "    def tf(word, doc):\n",
        "        return doc.count(word) / Counter(doc).most_common()[0][1]\n",
        "\n",
        "    def df(word, doclist):\n",
        "        return sum(1 for d in doclist if word in d)\n",
        "\n",
        "    def tfidf(word, doc, dfdict, N):\n",
        "        return tf(word, doc) * math.log10((N / dfdict[word]))\n",
        "\n",
        "    def getcsrmatrix(tokens,dfdict,N,vocab, dish_flavours, max_vocab):\n",
        "        matrixRow_list = []\n",
        "        if include_flavours:\n",
        "            matrixRow_list = np.zeros((1,len(vocab) + len(dish_flavours) - 1),dtype='float')\n",
        "        else:\n",
        "            matrixRow_list = np.zeros((1,len(vocab)),dtype='float')\n",
        "        for t in tokens:\n",
        "            if t in vocab:\n",
        "                matrixRow_list[0][vocab[t]] = tfidf(t,tokens,dfdict,N)\n",
        "\n",
        "        if include_flavours:\n",
        "            matrixRow_list[0][max_vocab] = dish_flavours['bitter']\n",
        "            matrixRow_list[0][max_vocab] = dish_flavours['rich']\n",
        "            matrixRow_list[0][max_vocab + 1] = dish_flavours['salt']\n",
        "            matrixRow_list[0][max_vocab + 3] = dish_flavours['spicy']\n",
        "            matrixRow_list[0][max_vocab + 2] = dish_flavours['sweet']\n",
        "            matrixRow_list[0][max_vocab + 5] = dish_flavours['umami']\n",
        "\n",
        "        return csr_matrix(matrixRow_list)\n",
        "\n",
        "    flavour = pd.read_csv(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tastes.csv'), names = ['dishId', 'bitter', 'rich', 'salt', 'spicy', 'sweet', 'umami'])\n",
        "\n",
        "    N=len(db)\n",
        "\n",
        "    doclist = db['tokens'].tolist()\n",
        "    vocab = { i:x for x,i in enumerate(sorted(list(set(i for s in doclist for i in s)))) }\n",
        "    max_vocab = max(vocab.values()) + 1\n",
        "    print(\"max vocab\",max_vocab)\n",
        "    dfdict = {}\n",
        "    for v in vocab.items():\n",
        "        dfdict[v[0]] = df(v[0],doclist)\n",
        "\n",
        "    csrlist = []\n",
        "    for index, row in db.iterrows():\n",
        "        dish_flavours = flavour[flavour.dishId == row['dishId']].to_dict(orient = 'record')[0]\n",
        "        csrlist.append(getcsrmatrix(row['tokens'],dfdict,N,vocab, dish_flavours, max_vocab)) # row['dishId'] and df with flavour scores\n",
        "\n",
        "    db['features'] =  csrlist\n",
        "    return (db,vocab)\n",
        "\n",
        "\n",
        "def my_train_test_split(ratings):\n",
        "    \"\"\"\n",
        "    Returns a random split of the ratings matrix into a training and testing set.\n",
        "    \"\"\"\n",
        "    train_set, test_set = train_test_split(ratings, test_size = 0.20, random_state = 42)\n",
        "    return train_set, test_set\n",
        "    '''\n",
        "    test = set(range(len(ratings))[::10])\n",
        "    train = sorted(set(range(len(ratings))) - test)\n",
        "    test = sorted(test)\n",
        "    return ratings.iloc[train], ratings.iloc[test]\n",
        "    '''\n",
        "\n",
        "def cosine_sim(a, b, include_flavours):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    v1 = a.toarray()[0]\n",
        "    v2  = b.toarray()[0]\n",
        "    def cos_sim(v1, v2):\n",
        "        x = (math.sqrt(sum([i*i for i in v1]))*math.sqrt(sum([i*i for i in v2])))\n",
        "        if x:\n",
        "            return sum(i[0] * i[1] for i in zip(v1, v2)) / x\n",
        "        else:\n",
        "            return 0\n",
        "    # s1 = cos_sim(v1, v2)\n",
        "    # return s1\n",
        "    s1 = cos_sim(v1[:-6], v2[:-6])\n",
        "    if include_flavours:\n",
        "        s2 = cos_sim(v1[-6:], v2[-6:])\n",
        "        return s1 * 0.5 + s2 * 0.5\n",
        "    else:\n",
        "        return s1\n",
        "\n",
        "def make_predictions(db, ratings_train, ratings_test, include_flavours):\n",
        "    \"\"\"\n",
        "    Using the ratings in ratings_train, prediction is made on the ratings for each\n",
        "    row in ratings_test.\n",
        "    This is done by computing the weighted average\n",
        "    rating for every other dish that the user has rated.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    x = 0\n",
        "    for index,row in ratings_test.iterrows():\n",
        "        # mlist contains dishIds rated by the user in the train set\n",
        "        mlist = list(ratings_train.loc[ratings_train['userId'] == row['userId']]['dishId'])\n",
        "        # csr list contains tfidf scores of tags for dishes rated by the user\n",
        "        csrlist = list(db.loc[db['dishId'].isin(mlist)]['features'])\n",
        "        # mrlist contains scores of dishes rated by the user (dishes in mlist)\n",
        "        mrlist = list(ratings_train.loc[ratings_train['userId'] == row['userId']]['rating'])\n",
        "        # computing similarity between dishes user rated and the current dish in the test set\n",
        "\n",
        "        sim = [cosine_sim(c,db.loc[db['dishId'] ==row['dishId']]['features'].values[0], include_flavours) for c in csrlist]\n",
        "        # computing similarity times the rating for known dish\n",
        "        wan = sum([ v*mrlist[i] for i,v in enumerate(sim) if v>0])\n",
        "        wadlist = [i for i in sim if i>0]\n",
        "        ## check for sum(wadlist) > 1\n",
        "        if len(wadlist)>0 and sum(wadlist) >= 1:\n",
        "            result.append(wan/sum(wadlist))\n",
        "            x = x + 1\n",
        "        else:\n",
        "            result.append(np.mean(mrlist)) # if dish did not match with anything approx as average of users rating\n",
        "    return np.array(result)\n",
        "\n",
        "def main(data, db, predict_on, include_flavours):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    total_dishes = db.shape[0]\n",
        "\n",
        "    db = tokenize(db)\n",
        "    db, vocab = featurize(db, include_flavours)\n",
        "    \n",
        "    ratings_train, ratings_test = my_train_test_split(data)\n",
        "    predictions = make_predictions(db, ratings_train, ratings_test, include_flavours)\n",
        "\n",
        "    predicted_test_error = mean_squared_error(ratings_test.rating, predictions) ** 0.5\n",
        "\n",
        "    def predict_on_user(predict_on):\n",
        "        ratings_test = pd.DataFrame(columns = ['userId', 'dishId'])\n",
        "        ratings_test['userId'] = [predict_on] * total_dishes\n",
        "        ratings_test.dishId = range(1, total_dishes + 1)\n",
        "           \n",
        "        predictions_uid = make_predictions(db, ratings_train, ratings_test, include_flavours)\n",
        "\n",
        "        predictions_uid = list(enumerate(predictions_uid))\n",
        "\n",
        "        predictions_uid = sorted(predictions_uid, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "        predictions_uid = list(map(lambda x: (x[0] + 1, x[1]), predictions_uid))\n",
        "\n",
        "        return predictions_uid\n",
        "\n",
        "    return (predicted_test_error, predict_on_user(predict_on = predict_on))\n",
        "    \n",
        "\n",
        "def start(profile = None, type = 'all', predict_on = 100, flavours = False, retrain = False):\n",
        "    time_start = time.time()\n",
        "    data = pd.read_csv(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/review.csv'))\n",
        "    data = data[data['userId'].isin(data['userId'].value_counts()[data['userId'].value_counts() >= 5].index)]\n",
        "    print(data.shape)\n",
        "\n",
        "    if not retrain:\n",
        "        if flavours:\n",
        "            final_scores = pickle.load(open(os.path.join(my_path,\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_flavour_scores.pickle\"), \"rb\" ))\n",
        "            predictions = final_scores[predict_on]\n",
        "\n",
        "        else:\n",
        "            final_scores = pickle.load(open(os.path.join(my_path,\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_scores.pickle\"), \"rb\" ))\n",
        "            predictions = final_scores[predict_on]\n",
        "\n",
        "        predicted_test_error = None\n",
        "\n",
        "    else:\n",
        "        if profile:\n",
        "            data = append_to_data(data, profile, predict_on)\n",
        "\n",
        "        if type == 'all':\n",
        "            db = pd.read_csv(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/meta_cuisine.csv'))\n",
        "        elif type == 'meta':\n",
        "            db = pd.read_csv(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/database.csv'), names = ['dishId', 'tags'])\n",
        "\n",
        "        dishes = pd.read_csv(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/id_name_mapping.csv'), names = ['dishId', 'dish_name'])\n",
        "\n",
        "        predicted_test_error, predictions = main(data, db, predict_on = predict_on, include_flavours = flavours)\n",
        "        \n",
        "        predictions = pd.DataFrame(predictions, columns = ['dishId', 'rating'])\n",
        "        predictions = predictions.merge(dishes, on = 'dishId', how = 'left')\n",
        "        predictions.columns = ['dishId', 'rating', 'dishName']\n",
        "\n",
        "        if flavours:\n",
        "            if os.path.exists(os.path.join(my_path,\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_flavour_scores.pickle\")):\n",
        "                final_scores = pickle.load(open(os.path.join(my_path,\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_flavour_scores.pickle\"), \"rb\" ))\n",
        "                final_scores[predict_on] = predictions\n",
        "            else:\n",
        "                final_scores = {}\n",
        "                final_scores[predict_on] = predictions\n",
        "\n",
        "            pickle.dump(final_scores, open(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_flavour_scores.pickle'), 'wb'))\n",
        "\n",
        "        else:\n",
        "            if os.path.exists(os.path.join(my_path,\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_scores.pickle\")):\n",
        "                final_scores = pickle.load(open(os.path.join(my_path,\"/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_scores.pickle\"), \"rb\" ))\n",
        "                final_scores[predict_on] = predictions\n",
        "\n",
        "            else:\n",
        "                final_scores = {}\n",
        "                final_scores[predict_on] = predictions\n",
        "\n",
        "            pickle.dump(final_scores, open(os.path.join(my_path,'/content/gdrive/My Drive/Final Year Project/ContentBasedFiltering/Utilities/Team 3/tfidf_final_scores.pickle'), 'wb'))\n",
        "\n",
        "    data = data[data.userId == predict_on]\n",
        "    original_rating = data.merge(predictions, how = 'left', on = 'dishId')\n",
        "    original_rating.columns = ['dishId', 'userId', 'rating', 'reformed', 'dishName']\n",
        "    \n",
        "    time_end = time.time()\n",
        "\n",
        "    answer = {\"user\" : predict_on, \"predicted_test_error\": predicted_test_error, \"time\" : round(time_end - time_start, 2), \"predicted_rating\" : predictions, \"original_rating\" : original_rating}\n",
        "    return answer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4fzSjoQ3W0o",
        "colab_type": "code",
        "outputId": "22b61acf-9ed0-47c2-aec4-15ddabe73f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "start(retrain=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4019, 3)\n",
            "max vocab 382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_rating':    dishId  userId  rating  reformed             dishName\n",
              " 0     276     100       5  4.333195          mango lassi\n",
              " 1      15     100       5  3.923997        chicken curry\n",
              " 2     176     100       4  4.444444        nariyal burfi\n",
              " 3      16     100       5  4.527952      chicken makhani\n",
              " 4     150     100       5  4.579875     strawberry lassi\n",
              " 5      13     100       5  4.756489     vegetarian korma\n",
              " 6      31     100       5  4.511668          mango lassi\n",
              " 7     206     100       3  4.561917                pulao\n",
              " 8      92     100       3  4.201004  curried cauliflower\n",
              " 9     245     100       4  4.511552          mango lassi,\n",
              " 'predicted_rating':       dishId    rating                               dishName\n",
              " 0         54  4.881832                           saffron rice\n",
              " 1         13  4.756489                       vegetarian korma\n",
              " 2        341  4.713186  saffron rice with raisins and cashews\n",
              " 3       1117  4.702834                           mexican rice\n",
              " 4        561  4.656520                      vegetable biryani\n",
              " ...      ...       ...                                    ...\n",
              " 1376      15  3.923997                          chicken curry\n",
              " 1377      52  3.912213                        chicken makhani\n",
              " 1378     564  3.908324                       chicken marinade\n",
              " 1379     297  3.882804                         seitan makhani\n",
              " 1380     583  3.799022               milk with turmeric paste\n",
              " \n",
              " [1381 rows x 3 columns],\n",
              " 'predicted_test_error': 1.0401240711605004,\n",
              " 'time': 35.1,\n",
              " 'user': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U17gYcSf4b2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}