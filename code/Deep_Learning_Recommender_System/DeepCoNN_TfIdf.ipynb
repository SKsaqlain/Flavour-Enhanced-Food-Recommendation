{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepCoNN-TfIdf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h99z3kwwjBAP",
        "colab_type": "code",
        "outputId": "daa1b478-de34-40b4-a29e-5e21db90c838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "# preprocessing imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.27.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wNIFYhEjKTV",
        "colab_type": "code",
        "outputId": "3118f319-436d-47b6-a378-9e64bcc9a035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYwoPFmnkW3b",
        "colab_type": "code",
        "outputId": "ac5d7f03-2d1d-4561-c3de-922bc38c69fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "rawData=pd.read_csv('/content/gdrive/My Drive/Final Year Project/DeepConn/Deep_Learning_Recommender_System/unembedded_grouped_cleaned_data.csv')\n",
        "rawData.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>user_id</th>\n",
              "      <th>food_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>userReviews</th>\n",
              "      <th>foodReviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9974</td>\n",
              "      <td>998</td>\n",
              "      <td>5</td>\n",
              "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
              "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10340</td>\n",
              "      <td>998</td>\n",
              "      <td>5</td>\n",
              "      <td>['feeling', 'healthy', 'great', 'replication',...</td>\n",
              "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>12047</td>\n",
              "      <td>998</td>\n",
              "      <td>5</td>\n",
              "      <td>['yum', 'will', 'make', 'again']</td>\n",
              "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>13451</td>\n",
              "      <td>998</td>\n",
              "      <td>5</td>\n",
              "      <td>['turned', 'out', 'great', 'i', 'added', 'pean...</td>\n",
              "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>18226</td>\n",
              "      <td>998</td>\n",
              "      <td>5</td>\n",
              "      <td>['fantastic', 'tasting', 'completely', 'simple...</td>\n",
              "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                 foodReviews\n",
              "0           0  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
              "1           1  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
              "2           2  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
              "3           3  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
              "4           8  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKDdDjz1rD_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train/test split for our model is unique, we need to hold out a\n",
        "# set of users and movies so that our network never learns those \n",
        "testSize = 0.005\n",
        "\n",
        "# get testSize percentage of users\n",
        "uniqueUsers = rawData.loc[:, \"user_id\"].unique()\n",
        "usersSize = len(uniqueUsers)\n",
        "test_idx = np.random.choice(usersSize,\n",
        "                              size=int(usersSize * testSize),\n",
        "                              replace=False)\n",
        "\n",
        "# get test users\n",
        "test_users = uniqueUsers[test_idx]\n",
        "\n",
        "# everyone else is a training user\n",
        "trainUsers = np.delete(uniqueUsers, test_idx)\n",
        "\n",
        "test = rawData[rawData[\"user_id\"].isin(test_users)]\n",
        "train = rawData[rawData[\"user_id\"].isin(trainUsers)]\n",
        "\n",
        "uniqueTestFood = test[\"food_id\"].unique()\n",
        "\n",
        "# drop the movies that also appear in our test set. In order to be\n",
        "# a true train/test split, we are forced to discard some data entirely\n",
        "train = train.where(np.logical_not(train[\"food_id\"].isin(uniqueTestFood))).dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueotcM2EiFOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "embeddingSize=241\n",
        "tfidf=TfidfVectorizer(max_features=241)\n",
        "tfidf.fit(list(train[\"userReviews\"]))\n",
        "tfidf_features=tfidf.transform(list(train[\"userReviews\"])).astype(\"float64\")\n",
        "\n",
        "trainUserReviews=tfidf_features.toarray()\n",
        "trainFoodReviews=tfidf.transform(list(train[\"foodReviews\"])).toarray()\n",
        "\n",
        "\n",
        "testUserReviews=tfidf.transform(list(test[\"userReviews\"])).toarray()\n",
        "testFoodReviews=tfidf.transform(list(test[\"foodReviews\"])).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3UEX5FDib-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24f6e720-f7b2-4a58-bcdf-705d88e83fb8"
      },
      "source": [
        "print(type(trainUserReviews),trainUserReviews.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (15669, 241)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX5JjU6TkJ5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==1.14.0\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from keras.layers import Input, Dense , Embedding\n",
        "from keras.layers.merge import Add, Dot, Concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tVa3SbwjLv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepCoNN():\n",
        "    def __init__(self,\n",
        "                 embeddingSize,\n",
        "                 hiddenSize,\n",
        "                 filters=2,\n",
        "                 kernelSize=10,\n",
        "                 strides=6):\n",
        "        self.embeddingSize = embeddingSize\n",
        "        self.hiddenSize = hiddenSize\n",
        "        self.filters = filters\n",
        "        self.kernelSize = kernelSize\n",
        "        self.inputU, self.towerU = self.createDeepCoNnTower()\n",
        "        self.inputM, self.towerM = self.createDeepCoNnTower()\n",
        "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
        "        self.outNeuron = Dense(1)(self.joined)\n",
        "\n",
        "    def createDeepCoNnTower(self):\n",
        "        inputLayer = Input(shape=(self.embeddingSize,))\n",
        "        embeddingLayer=Embedding(self.embeddingSize,100)(inputLayer)\n",
        "        tower = Conv1D(filters=self.filters,\n",
        "                       kernel_size=self.kernelSize,\n",
        "                       activation=\"tanh\")(embeddingLayer)\n",
        "        tower = MaxPooling1D()(tower)\n",
        "        tower = Flatten()(tower)\n",
        "        tower = Dense(self.hiddenSize, activation=\"relu\")(tower)\n",
        "        return inputLayer, tower\n",
        "\n",
        "    def createDeepCoNnDp(self):\n",
        "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
        "        output = Add()([self.outNeuron, dotproduct])\n",
        "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
        "        self.model.compile(optimizer='Adam', loss='mse')\n",
        "        \n",
        "    def train(self, trainData,trainUserReviews,trainFoodReviews, batch_size, epochs=1):\n",
        "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(pd.Timestamp(int(time()), unit=\"s\")))\n",
        "        self.createDeepCoNnDp()\n",
        "        print(self.model.summary())\n",
        "        \n",
        "        # userReviews = np.array(trainData.loc[:, \"userReviews\"])\n",
        "        # foodReviews = np.array(trainData.loc[:, \"foodReviews\"])\n",
        "        userReviews=trainUserReviews\n",
        "        foodReviews=trainFoodReviews\n",
        "\n",
        "        self.train_inputs = [userReviews, foodReviews]\n",
        "        self.train_outputs = trainData.loc[:, \"rating\"]\n",
        "        \n",
        "        self.history = self.model.fit(self.train_inputs,\n",
        "                                      self.train_outputs,\n",
        "                                      callbacks=[tensorboard],\n",
        "                                      validation_split=0.05,\n",
        "                                      batch_size=batch_size,\n",
        "                                      epochs=epochs)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XADlZUDPjhea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ac12c3a-b857-4c34-8ed1-6db80e4d7c67"
      },
      "source": [
        "hiddenSize = 64\n",
        "\n",
        "deepconn = DeepCoNN(trainUserReviews.shape[1], hiddenSize)\n",
        "\n",
        "batch_size = 32\n",
        "deepconn.train(train,trainUserReviews,trainFoodReviews, batch_size, epochs=10)\n",
        "\n",
        "deepconn.model.save(\"cnn.h5\")\n",
        "# print(train_embedded.loc[0]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           (None, 241)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           (None, 241)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 241, 100)     24100       input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, 241, 100)     24100       input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 232, 2)       2002        embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 232, 2)       2002        embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling1D) (None, 116, 2)       0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling1D) (None, 116, 2)       0           conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 232)          0           max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 232)          0           max_pooling1d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 64)           14912       flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 64)           14912       flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 128)          0           dense_16[0][0]                   \n",
            "                                                                 dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1)            129         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dot_5 (Dot)                     (None, 1)            0           dense_16[0][0]                   \n",
            "                                                                 dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 1)            0           dense_18[0][0]                   \n",
            "                                                                 dot_5[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 82,157\n",
            "Trainable params: 82,157\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 14885 samples, validate on 784 samples\n",
            "Epoch 1/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.9312 - val_loss: 0.6895\n",
            "Epoch 2/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6727 - val_loss: 0.7218\n",
            "Epoch 3/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6641 - val_loss: 0.7540\n",
            "Epoch 4/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6670 - val_loss: 0.7675\n",
            "Epoch 5/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6670 - val_loss: 0.6794\n",
            "Epoch 6/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6647 - val_loss: 0.8277\n",
            "Epoch 7/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6795 - val_loss: 0.8770\n",
            "Epoch 8/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6655 - val_loss: 0.7455\n",
            "Epoch 9/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6638 - val_loss: 0.6863\n",
            "Epoch 10/10\n",
            "14885/14885 [==============================] - 40s 3ms/step - loss: 0.6635 - val_loss: 0.6786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmJqoO5oj9du",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "7a2cce62-6318-44c4-bea3-50dbffff8562"
      },
      "source": [
        "\n",
        "testInputs = [testUserReviews, testFoodReviews]\n",
        "print(testInputs)\n",
        "# dat = pd.DataFrame(testInputs)\n",
        "# dat.to_csv(\"/content/gdrive/My Drive/DeepConn/Deep_Learning_Recommender_System//test_data.csv\")\n",
        "\n",
        "trueRating = np.array(list(test.loc[:, \"rating\"])).reshape((-1, 1))\n",
        "\n",
        "predictions = deepconn.model.predict(testInputs)\n",
        "\n",
        "error = np.square(predictions - trueRating)\n",
        "\n",
        "print(\"MSE:\", np.average(error))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.45865294],\n",
            "       ...,\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
            "MSE: 0.5959017717145865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h770f3S0p6K0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}