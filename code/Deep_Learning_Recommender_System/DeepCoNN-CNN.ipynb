{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhpsGCZwOSQh"
   },
   "outputs": [],
   "source": [
    "# preprocessing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29655,
     "status": "ok",
     "timestamp": 1583945306361,
     "user": {
      "displayName": "Sumair Shariff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitHIRru0c6F8QLTRRCPOgboa39qFgHCKEuUUd7IA=s64",
      "userId": "12669468645809207600"
     },
     "user_tz": -330
    },
    "id": "oCl-o-SSfrB6",
    "outputId": "c4828bfd-e16f-4cd3-e818-051cfe6313bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2273,
     "status": "ok",
     "timestamp": 1583947463654,
     "user": {
      "displayName": "Sumair Shariff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitHIRru0c6F8QLTRRCPOgboa39qFgHCKEuUUd7IA=s64",
      "userId": "12669468645809207600"
     },
     "user_tz": -330
    },
    "id": "tXwwPp-lfuxG",
    "outputId": "7223b321-9e5c-4f1f-8673-ac5911cd3955"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>food_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>foodReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9974</td>\n",
       "      <td>998</td>\n",
       "      <td>5</td>\n",
       "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
       "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10340</td>\n",
       "      <td>998</td>\n",
       "      <td>5</td>\n",
       "      <td>['feeling', 'healthy', 'great', 'replication',...</td>\n",
       "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12047</td>\n",
       "      <td>998</td>\n",
       "      <td>5</td>\n",
       "      <td>['yum', 'will', 'make', 'again']</td>\n",
       "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13451</td>\n",
       "      <td>998</td>\n",
       "      <td>5</td>\n",
       "      <td>['turned', 'out', 'great', 'i', 'added', 'pean...</td>\n",
       "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>18226</td>\n",
       "      <td>998</td>\n",
       "      <td>5</td>\n",
       "      <td>['fantastic', 'tasting', 'completely', 'simple...</td>\n",
       "      <td>['delicious', 'and', 'easy', 'to', 'make']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                 foodReviews\n",
       "0           0  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
       "1           1  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
       "2           2  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
       "3           3  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
       "4           8  ...  ['delicious', 'and', 'easy', 'to', 'make']\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data=pd.read_csv('/content/gdrive/My Drive/Final Year Project/DeepConn/Deep_Learning_Recommender_System/unembedded_grouped_cleaned_data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTC3vZXDPDeL"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init_embeddings_map(fname):\n",
    "    with open(fname) as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in\n",
    "                [line.split() for line in glove]}\n",
    "\n",
    "\n",
    "def get_embed_and_pad_func(i_seq_len, u_seq_len, pad_value, embedding_map):\n",
    "    def embed(row):\n",
    "        sentence = row[\"userReviews\"].split()[:u_seq_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word)\n",
    "            if word in embedding_map else pad_value, sentence))\n",
    "        row[\"userReviews\"] = reviews +\\\n",
    "                [pad_value] * (u_seq_len - len(reviews))\n",
    "        sentence = row[\"foodReviews\"].split()[:i_seq_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word)\n",
    "            if word in embedding_map else pad_value, sentence))\n",
    "        row[\"foodReviews\"] = reviews +\\\n",
    "                [pad_value] * (i_seq_len - len(reviews))\n",
    "        return row\n",
    "    return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gz10ArRLOSQ1"
   },
   "outputs": [],
   "source": [
    "# functions we implemented\n",
    "# from custom_functions import init_embeddings_map, get_embed_and_pad_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQccXSda3s3d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# link=\"https://drive.google.com/open?id=1AaNzc7XvqG6RiIVeYHDg10HuwTcDyutl\"\n",
    "# # emb_size = 50\n",
    "# # embedding_map = init_embeddings_map(\"glove.6B.\" + str(emb_size) + \"d.txt\")\n",
    "# downloaded = drive.CreateFile({'id':\"1AaNzc7XvqG6RiIVeYHDg10HuwTcDyutl\"}) \n",
    "# downloaded.GetContentFile('glove.6B.50d.txt')\n",
    "embedding_map = init_embeddings_map('/content/gdrive/My Drive/DeepConn/Deep_Learning_Recommender_System/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pds_zDlWOSRZ"
   },
   "outputs": [],
   "source": [
    "# Train/test split for our model is unique, we need to hold out a\n",
    "# set of users and movies so that our network never learns those \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = raw_data.loc[:, \"user_id\"].unique()\n",
    "users_size = len(unique_users)\n",
    "test_idx = np.random.choice(users_size,\n",
    "                              size=int(users_size * test_size),\n",
    "                              replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = raw_data[raw_data[\"user_id\"].isin(test_users)]\n",
    "train = raw_data[raw_data[\"user_id\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"food_id\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"food_id\"].isin(unique_test_movies))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc1wYf4wOSRk"
   },
   "outputs": [],
   "source": [
    "user_seq_sizes = raw_data.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_seq_sizes = raw_data.loc[:, \"foodReviews\"].apply(lambda x: x.split()).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3njyJoDVOSRs"
   },
   "outputs": [],
   "source": [
    "u_ptile = 40\n",
    "i_ptile = 30\n",
    "u_seq_len = int(np.percentile(user_seq_sizes, u_ptile))\n",
    "i_seq_len = int(np.percentile(item_seq_sizes, i_ptile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wEuoqohOSRz"
   },
   "outputs": [],
   "source": [
    "embedding_fn = get_embed_and_pad_func(i_seq_len, u_seq_len, np.array([0.0] * 50), embedding_map)\n",
    "    \n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDNPPh2DOSSA"
   },
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDiEt5C5OSSE"
   },
   "outputs": [],
   "source": [
    "# modeling imports\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Add, Dot, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPWgO3cyOSSL"
   },
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self,\n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 u_seq_len,\n",
    "                 m_seq_len,\n",
    "                 filters=2,\n",
    "                 kernel_size=10,\n",
    "                 strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = Conv1D(filters=self.filters,\n",
    "                       kernel_size=self.kernel_size,\n",
    "                       activation=\"tanh\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, train_data, batch_size, epochs=3500):\n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(pd.Timestamp(int(time()), unit=\"s\")))\n",
    "        self.create_deepconn_dp()\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        user_reviews = np.array(list(train_data.loc[:, \"userReviews\"]))\n",
    "        movie_reviews = np.array(list(train_data.loc[:, \"foodReviews\"]))\n",
    "\n",
    "        self.train_inputs = [user_reviews, movie_reviews]\n",
    "        self.train_outputs = train_data.loc[:, \"rating\"]\n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs,\n",
    "                                      self.train_outputs,\n",
    "                                      callbacks=[tensorboard],\n",
    "                                      validation_split=0.05,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=epochs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1583947664207,
     "user": {
      "displayName": "Sumair Shariff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitHIRru0c6F8QLTRRCPOgboa39qFgHCKEuUUd7IA=s64",
      "userId": "12669468645809207600"
     },
     "user_tz": -330
    },
    "id": "qABJ5ftNjLeL",
    "outputId": "e5c943d8-6742-40a1-fb92-bf9ce011c083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "11\n",
      "11\n",
      "Unnamed: 0                                                     0\n",
      "user_id                                                     9974\n",
      "food_id                                                      998\n",
      "rating                                                         5\n",
      "userReviews    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "foodReviews    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(hidden_size)\n",
    "print(u_seq_len)\n",
    "print(i_seq_len)\n",
    "print(train_embedded.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3869,
     "status": "ok",
     "timestamp": 1583947677170,
     "user": {
      "displayName": "Sumair Shariff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitHIRru0c6F8QLTRRCPOgboa39qFgHCKEuUUd7IA=s64",
      "userId": "12669468645809207600"
     },
     "user_tz": -330
    },
    "id": "Mn0xcanXOSSW",
    "outputId": "5a997654-6309-49fb-fc35-011304957216",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 11, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 11, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 2, 2)         1002        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 2, 2)         1002        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 1, 2)         0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 1, 2)         0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2)            0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 2)            0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           192         flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 64)           192         flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            129         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1)            0           dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 1)            0           dense_16[0][0]                   \n",
      "                                                                 dot_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,517\n",
      "Trainable params: 2,517\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 12082 samples, validate on 636 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n",
      "12082/12082 [==============================] - 1s 106us/step - loss: 5.4707 - val_loss: 0.7372\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "deepconn = DeepCoNN(50, hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "batch_size = 32\n",
    "deepconn.train(train_embedded, batch_size, epochs=1)\n",
    "\n",
    "deepconn.model.save(\"cnn.h5\")\n",
    "# print(train_embedded.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1110,
     "status": "ok",
     "timestamp": 1583947969451,
     "user": {
      "displayName": "Sumair Shariff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitHIRru0c6F8QLTRRCPOgboa39qFgHCKEuUUd7IA=s64",
      "userId": "12669468645809207600"
     },
     "user_tz": -330
    },
    "id": "qApX13u8OSSg",
    "outputId": "0853f7f2-7c8d-4435-fc96-eda8481b20b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]]), array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]])]\n",
      "MSE: 0.9106456193829169\n"
     ]
    }
   ],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"foodReviews\"]))\n",
    "\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "print(test_inputs)\n",
    "# dat = pd.DataFrame(test_inputs)\n",
    "# dat.to_csv(\"/content/gdrive/My Drive/DeepConn/Deep_Learning_Recommender_System//test_data.csv\")\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"rating\"])).reshape((-1, 1))\n",
    "\n",
    "predictions = deepconn.model.predict(test_inputs)\n",
    "\n",
    "error = np.square(predictions - true_rating)\n",
    "\n",
    "print(\"MSE:\", np.average(error))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepCoNN-CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
