{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_data(data, profile, predict_on):\n",
    "    profile = json.loads(profile)\n",
    "    dish_ids = list(map(int, profile.keys()))\n",
    "    ratings = list(map(int, profile.values()))\n",
    "\n",
    "    d = pd.DataFrame(columns = ['dishId', 'userId', 'rating'])\n",
    "    d['dishId'] = dish_ids\n",
    "    d['rating'] = ratings\n",
    "    d['userId'] = predict_on\n",
    "\n",
    "    data = data.append(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize(db):\n",
    "    \n",
    "    tokenlist=[]\n",
    "    for index,row in db.iterrows():\n",
    "        tokenlist.append(row.tags.lower().split(\"|\"))\n",
    "    db['tokens']=tokenlist\n",
    "    return db\n",
    "def featurize(db, include_flavours):\n",
    "    \n",
    "    def tf(word, doc):\n",
    "        return doc.count(word) / Counter(doc).most_common()[0][1]\n",
    "\n",
    "    def df(word, doclist):\n",
    "        return sum(1 for d in doclist if word in d)\n",
    "\n",
    "    def tfidf(word, doc, dfdict, N):\n",
    "        return tf(word, doc) * math.log10((N / dfdict[word]))\n",
    "\n",
    "    def getcsrmatrix(tokens,dfdict,N,vocab, dish_flavours, max_vocab):\n",
    "        matrixRow_list = []\n",
    "        if include_flavours:\n",
    "            matrixRow_list = np.zeros((1,len(vocab) + len(dish_flavours) - 1),dtype='float')\n",
    "        else:\n",
    "            matrixRow_list = np.zeros((1,len(vocab)),dtype='float')\n",
    "        for t in tokens:\n",
    "            if t in vocab:\n",
    "                matrixRow_list[0][vocab[t]] = tfidf(t,tokens,dfdict,N)\n",
    "\n",
    "        if include_flavours:\n",
    "            matrixRow_list[0][max_vocab] = dish_flavours['bitter']\n",
    "            matrixRow_list[0][max_vocab] = dish_flavours['rich']\n",
    "            matrixRow_list[0][max_vocab + 1] = dish_flavours['salt']\n",
    "            matrixRow_list[0][max_vocab + 3] = dish_flavours['spicy']\n",
    "            matrixRow_list[0][max_vocab + 2] = dish_flavours['sweet']\n",
    "            matrixRow_list[0][max_vocab + 5] = dish_flavours['umami']\n",
    "\n",
    "        return csr_matrix(matrixRow_list)\n",
    "\n",
    "    flavour = pd.read_csv('../dataset/Utilities/tastes.csv', names = ['dishId', 'bitter', 'rich', 'salt', 'spicy', 'sweet', 'umami'])\n",
    "    print(\"flavour shape= \",flavour.shape)\n",
    "    print(flavour.head())\n",
    "    \n",
    "    N=len(db)\n",
    "\n",
    "    doclist = db['tokens'].tolist()\n",
    "    #print(\"tokens = \",doclist[:6])\n",
    "    \n",
    "    vocab = { i:x for x,i in enumerate(sorted(list(set(i for s in doclist for i in s)))) }\n",
    "    #print(vocab)\n",
    "    max_vocab = max(vocab.values()) + 1\n",
    "    print(\"max vocab= \",max_vocab)\n",
    "    \n",
    "    dfdict = {}\n",
    "    for v in vocab.items():\n",
    "        dfdict[v[0]] = df(v[0],doclist)\n",
    "\n",
    "    csrlist = []\n",
    "    for index, row in db.iterrows():\n",
    "        dish_flavours = flavour[flavour.dishId == row['dishId']].to_dict(orient = 'record')[0]\n",
    "        csrlist.append(getcsrmatrix(row['tokens'],dfdict,N,vocab, dish_flavours, max_vocab)) # row['dishId'] and df with flavour scores\n",
    "\n",
    "    db['features'] =  csrlist\n",
    "    print(\"after including ifidf features\")\n",
    "    print(db.head())\n",
    "    return (db,vocab)\n",
    "\n",
    "def my_train_test_split(ratings):\n",
    "    \n",
    "    train_set, test_set = train_test_split(ratings, test_size = 0.20, random_state = 42)\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def cosine_sim(a, b, include_flavours):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    v1 = a.toarray()[0]\n",
    "    v2  = b.toarray()[0]\n",
    "    def cos_sim(v1, v2):\n",
    "        x = (math.sqrt(sum([i*i for i in v1]))*math.sqrt(sum([i*i for i in v2])))\n",
    "        if x:\n",
    "            return sum(i[0] * i[1] for i in zip(v1, v2)) / x\n",
    "        else:\n",
    "            return 0\n",
    "    # s1 = cos_sim(v1, v2)\n",
    "    # return s1\n",
    "    '''\n",
    "    s1 = cos_sim(v1[:-6], v2[:-6])\n",
    "    if include_flavours:\n",
    "        s2 = cos_sim(v1[-6:], v2[-6:])\n",
    "        return s1 * 0.5 + s2 * 0.5\n",
    "    else:\n",
    "        return s1\n",
    "    '''\n",
    "    s1=cos_sim(v1,v2)\n",
    "    return s1\n",
    "\n",
    "def make_predictions(db, ratings_train, ratings_test, include_flavours):\n",
    "    \n",
    "    result = []\n",
    "    x = 0\n",
    "    for index,row in ratings_test.iterrows():\n",
    "        # mlist contains dishIds rated by the user in the train set\n",
    "        mlist = list(ratings_train.loc[ratings_train['userId'] == row['userId']]['dishId'])\n",
    "        #print(\"dishes rated by user \",row[\"userId\"],\" \",mlist)\n",
    "        # csr list contains tfidf scores of tags for dishes rated by the user\n",
    "        csrlist = list(db.loc[db['dishId'].isin(mlist)]['features'])\n",
    "        #print(\"csrlist \",csrlist)\n",
    "        # mrlist contains scores of dishes rated by the user (dishes in mlist)\n",
    "        mrlist = list(ratings_train.loc[ratings_train['userId'] == row['userId']]['rating'])\n",
    "        #print(\"mrlist \",mrlist)\n",
    "        # computing similarity between dishes user rated and the current dish in the test set\n",
    "        l=[0]*len(db[\"dishId\"])\n",
    "        for i,ele in enumerate(db[\"dishId\"]):\n",
    "            if(int(ele)==int(row[\"dishId\"])):\n",
    "                l[i]=1\n",
    "        sim = [cosine_sim(c,db.loc[l]['features'].values[0], include_flavours) for c in csrlist]\n",
    "        # computing similarity times the rating for known dish\n",
    "        wan = sum([ v*mrlist[i] for i,v in enumerate(sim) if v>0])\n",
    "        wadlist = [i for i in sim if i>0]\n",
    "        ## check for sum(wadlist) > 1\n",
    "        if len(wadlist)>0 and sum(wadlist) >= 1:\n",
    "            result.append(wan/sum(wadlist))\n",
    "            x = x + 1\n",
    "        else:\n",
    "            result.append(np.mean(mrlist)) # if dish did not match with anything approx as average of users rating\n",
    "    return np.array(result)\n",
    "\n",
    "def main(data, db, predict_on, include_flavours):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    total_dishes = db.shape[0]\n",
    "    print(\"In main total dishes= \",total_dishes)\n",
    "    \n",
    "    db = tokenize(db)\n",
    "    print(\"after tokenizing db= \",db.shape)\n",
    "    print(db.head())\n",
    "    \n",
    "    db, vocab = featurize(db, include_flavours)\n",
    "    \n",
    "    ratings_train, ratings_test = my_train_test_split(data)\n",
    "    print(\"trainig shape= \",ratings_train.shape)\n",
    "    print(\"testing shape= \",ratings_test.shape)\n",
    "\n",
    "    predictions = make_predictions(db, ratings_train, ratings_test, include_flavours)\n",
    "\n",
    "    predicted_test_error = mean_squared_error(ratings_test.rating, predictions) ** 0.5\n",
    "\n",
    "    def predict_on_user(predict_on):\n",
    "        ratings_test = pd.DataFrame(columns = ['userId', 'dishId'])\n",
    "        ratings_test['userId'] = [predict_on] * total_dishes\n",
    "        ratings_test.dishId = range(1, total_dishes + 1)\n",
    "           \n",
    "        predictions_uid = make_predictions(db, ratings_train, ratings_test, include_flavours)\n",
    "\n",
    "        predictions_uid = list(enumerate(predictions_uid))\n",
    "\n",
    "        predictions_uid = sorted(predictions_uid, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        predictions_uid = list(map(lambda x: (x[0] + 1, x[1]), predictions_uid))\n",
    "\n",
    "        return predictions_uid\n",
    "\n",
    "    return (predicted_test_error, predict_on_user(predict_on = predict_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(profile = None, type = 'meta', predict_on = 9974, flavours = False, retrain = False):\n",
    "    time_start = time.time()\n",
    "    data = pd.read_csv('../dataset/Utilities/newReview.csv')\n",
    "    data=data.mask(data.eq('None')).dropna()\n",
    "    print(\"before elimination =\",data.shape)\n",
    "    print(data.head())\n",
    "    data = data[data['userId'].isin(data['userId'].value_counts()[data['userId'].value_counts() >= 5].index)]\n",
    "    print(\"after elimination =\",data.shape)\n",
    "\n",
    "    if not retrain:\n",
    "        if flavours:\n",
    "            final_scores = pickle.load(open(\"../dataset/Utilities/tfidf_final_flavour_scores.pickle\"), \"rb\" )\n",
    "            predictions = final_scores[predict_on]\n",
    "\n",
    "        else:\n",
    "            final_scores = pickle.load(open(\"../dataset/Utilities/tfidf_final_scores.pickle\"), \"rb\" )\n",
    "            predictions = final_scores[predict_on]\n",
    "\n",
    "        predicted_test_error = None\n",
    "\n",
    "    else:\n",
    "        if profile:\n",
    "            data = append_to_data(data, profile, predict_on)\n",
    "\n",
    "        if type == 'all':\n",
    "            db = pd.read_csv('../dataset/Utilities/meta_cuisine.csv')\n",
    "        elif type == 'meta':\n",
    "            db = pd.read_csv('../dataset/Utilities/newDatabase.csv', names = ['dishId', 'tags'])\n",
    "        db=db.dropna()\n",
    "        print(\"food db size= \",db.shape)\n",
    "        print(db.head())\n",
    "        \n",
    "        dishes = pd.read_csv('../dataset/Utilities/id_name_mapping.csv', names = ['dishId', 'dish_name'])\n",
    "        dishes=dishes.dropna()\n",
    "        print(\"dishes size= \",dishes.shape)\n",
    "        print(dishes.head())\n",
    "        \n",
    "        predicted_test_error, predictions = main(data, db, predict_on = predict_on, include_flavours = flavours)\n",
    "        \n",
    "        predictions = pd.DataFrame(predictions, columns = ['dishId', 'rating'])\n",
    "        predictions = predictions.merge(dishes, on = 'dishId', how = 'left')\n",
    "        predictions.columns = ['dishId', 'rating', 'dishName']\n",
    "\n",
    "        if flavours:\n",
    "            if os.path.exists(\"../dataset/Utilities/tfidf_final_flavour_scores.pickle\"):\n",
    "                final_scores = pickle.load(open(\"../dataset/Utilities/tfidf_final_flavour_scores.pickle\", \"rb\" ))\n",
    "                final_scores[predict_on] = predictions\n",
    "            else:\n",
    "                final_scores = {}\n",
    "                final_scores[predict_on] = predictions\n",
    "\n",
    "            pickle.dump(final_scores, open('../dataset/Utilities/tfidf_final_flavour_scores.pickle', 'wb'))\n",
    "\n",
    "        else:\n",
    "            if os.path.exists(\"../dataset/Utilities/tfidf_final_scores.pickle\"):\n",
    "                final_scores = pickle.load(open(\"../dataset/Utilities/tfidf_final_scores.pickle\", \"rb\" ))\n",
    "                final_scores[predict_on] = predictions\n",
    "\n",
    "            else:\n",
    "                final_scores = {}\n",
    "                final_scores[predict_on] = predictions\n",
    "\n",
    "            pickle.dump(final_scores, open('../dataset/Utilities/tfidf_final_scores.pickle', 'wb'))\n",
    "\n",
    "    data = data[data.userId == predict_on]\n",
    "    data[\"dishId\"]=data[\"dishId\"].astype(np.int64)\n",
    "    original_rating = data.merge(predictions, how = 'left', on = 'dishId')\n",
    "    original_rating.columns = ['dishId', 'userId', 'rating', 'reformed', 'dishName']\n",
    "    \n",
    "    time_end = time.time()\n",
    "\n",
    "    answer = {\"user\" : predict_on, \"predicted_test_error\": predicted_test_error, \"time\" : round(time_end - time_start, 2), \"predicted_rating\" : predictions, \"original_rating\" : original_rating}\n",
    "    #answer = {\"user\" : predict_on, \"predicted_test_error\": predicted_test_error, \"time\" : round(time_end - time_start, 2)}\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SKsaqlain\\Anaconda3\\envs\\fyp\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before elimination = (106489, 3)\n",
      "  dishId  userId  rating\n",
      "0    998    9974       5\n",
      "1    998   10340       5\n",
      "2    998   12047       5\n",
      "3    998   13451       5\n",
      "4    998    9974       5\n",
      "after elimination = (69913, 3)\n",
      "food db size=  (1380, 2)\n",
      "   dishId                                               tags\n",
      "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...\n",
      "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...\n",
      "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...\n",
      "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...\n",
      "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...\n",
      "dishes size=  (1381, 2)\n",
      "   dishId                  dish_name\n",
      "0       1   curried green bean salad\n",
      "1       2                 keema aloo\n",
      "2       3                    paratha\n",
      "3       4    black chana with potato\n",
      "4       5  tomato cucumber kachumbar\n",
      "In main total dishes=  1380\n",
      "after tokenizing db=  (1380, 3)\n",
      "   dishId                                               tags  \\\n",
      "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...   \n",
      "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...   \n",
      "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...   \n",
      "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...   \n",
      "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [ethyl lactate, 3,4-dihydroxybenzaldehyde, dl-...  \n",
      "1  [ac1ldi49, 56424-87-4, 3,4-dihydroxybenzaldehy...  \n",
      "2  [3-methyl-1-butanol, thymol, 2-nonanone, pyrro...  \n",
      "3  [ac1ldi49, 56424-87-4, 2-hexenyl propanoate, 3...  \n",
      "4  [3,4-dihydroxybenzaldehyde, dl-liquiritigenin,...  \n",
      "flavour shape=  (1381, 7)\n",
      "   dishId  bitter  rich   salt  spicy  sweet  umami\n",
      "0       1   0.961  0.71  4.567   5.20   3.84      1\n",
      "1       2   3.876  4.50  0.240   4.56   0.38      8\n",
      "2       3   0.000  2.00  2.725  10.00   0.14      0\n",
      "3       4   4.672  0.87  0.294   3.37   1.91      6\n",
      "4       5   0.813  0.00  6.173   8.02   3.23      6\n",
      "max vocab=  1405\n",
      "after including ifidf features\n",
      "   dishId                                               tags  \\\n",
      "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...   \n",
      "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...   \n",
      "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...   \n",
      "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...   \n",
      "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [ethyl lactate, 3,4-dihydroxybenzaldehyde, dl-...   \n",
      "1  [ac1ldi49, 56424-87-4, 3,4-dihydroxybenzaldehy...   \n",
      "2  [3-methyl-1-butanol, thymol, 2-nonanone, pyrro...   \n",
      "3  [ac1ldi49, 56424-87-4, 2-hexenyl propanoate, 3...   \n",
      "4  [3,4-dihydroxybenzaldehyde, dl-liquiritigenin,...   \n",
      "\n",
      "                                            features  \n",
      "0    (0, 0)\\t0.44264974364151893\\n  (0, 1)\\t0.108...  \n",
      "1    (0, 0)\\t0.44264974364151893\\n  (0, 1)\\t0.108...  \n",
      "2    (0, 107)\\t0.04436104407808569\\n  (0, 108)\\t0...  \n",
      "3    (0, 0)\\t0.44264974364151893\\n  (0, 1)\\t0.108...  \n",
      "4    (0, 3)\\t0.024601495005335164\\n  (0, 9)\\t0.02...  \n",
      "trainig shape=  (55930, 3)\n",
      "testing shape=  (13983, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SKsaqlain\\Anaconda3\\envs\\fyp\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\SKsaqlain\\Anaconda3\\envs\\fyp\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user': 100,\n",
       " 'predicted_test_error': 0.49264960426279847,\n",
       " 'time': 1355.84,\n",
       " 'predicted_rating':       dishId  rating                      dishName\n",
       " 0          1     NaN      curried green bean salad\n",
       " 1          2     NaN                    keema aloo\n",
       " 2          3     NaN                       paratha\n",
       " 3          4     NaN       black chana with potato\n",
       " 4          5     NaN     tomato cucumber kachumbar\n",
       " ...      ...     ...                           ...\n",
       " 1375    1376     NaN              watermelon juice\n",
       " 1376    1377     NaN              watermelon juice\n",
       " 1377    1378     NaN         white sauce for pasta\n",
       " 1378    1379     NaN         white sauce for pasta\n",
       " 1379    1380     NaN  white sauce recipe for pasta\n",
       " \n",
       " [1380 rows x 3 columns],\n",
       " 'original_rating': Empty DataFrame\n",
       " Columns: [dishId, userId, rating, reformed, dishName]\n",
       " Index: []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start(retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SKsaqlain\\Anaconda3\\envs\\fyp\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before elimination = (106489, 3)\n",
      "  dishId  userId  rating\n",
      "0    998    9974       5\n",
      "1    998   10340       5\n",
      "2    998   12047       5\n",
      "3    998   13451       5\n",
      "4    998    9974       5\n",
      "after elimination = (69913, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/Utilities/newReview.csv')\n",
    "data=data.mask(data.eq('None')).dropna()\n",
    "print(\"before elimination =\",data.shape)\n",
    "print(data.head())\n",
    "data = data[data['userId'].isin(data['userId'].value_counts()[data['userId'].value_counts() >= 5].index)]\n",
    "print(\"after elimination =\",data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food db size=  (1380, 2)\n",
      "   dishId                                               tags\n",
      "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...\n",
      "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...\n",
      "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...\n",
      "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...\n",
      "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...\n"
     ]
    }
   ],
   "source": [
    "profile = None;type = 'meta';predict_on = 9974;flavours = False;retrain = True\n",
    "\n",
    "if profile:\n",
    "    data = append_to_data(data, profile, predict_on)\n",
    "\n",
    "if type == 'all':\n",
    "    db = pd.read_csv('../dataset/Utilities/meta_cuisine.csv')\n",
    "elif type == 'meta':\n",
    "    db = pd.read_csv('../dataset/Utilities/newDatabase.csv', names = ['dishId', 'tags'])\n",
    "db=db.dropna()\n",
    "print(\"food db size= \",db.shape)\n",
    "print(db.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dishes size=  (1381, 2)\n",
      "   dishId                  dish_name\n",
      "0       1   curried green bean salad\n",
      "1       2                 keema aloo\n",
      "2       3                    paratha\n",
      "3       4    black chana with potato\n",
      "4       5  tomato cucumber kachumbar\n"
     ]
    }
   ],
   "source": [
    "dishes = pd.read_csv('../dataset/Utilities/id_name_mapping.csv', names = ['dishId', 'dish_name'])\n",
    "dishes=dishes.dropna()\n",
    "print(\"dishes size= \",dishes.shape)\n",
    "print(dishes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In main total dishes=  1380\n",
      "after tokenizing db=  (1380, 3)\n",
      "   dishId                                               tags  \\\n",
      "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...   \n",
      "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...   \n",
      "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...   \n",
      "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...   \n",
      "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [ethyl lactate, 3,4-dihydroxybenzaldehyde, dl-...  \n",
      "1  [ac1ldi49, 56424-87-4, 3,4-dihydroxybenzaldehy...  \n",
      "2  [3-methyl-1-butanol, thymol, 2-nonanone, pyrro...  \n",
      "3  [ac1ldi49, 56424-87-4, 2-hexenyl propanoate, 3...  \n",
      "4  [3,4-dihydroxybenzaldehyde, dl-liquiritigenin,...  \n",
      "flavour shape=  (1381, 7)\n",
      "   dishId  bitter  rich   salt  spicy  sweet  umami\n",
      "0       1   0.961  0.71  4.567   5.20   3.84      1\n",
      "1       2   3.876  4.50  0.240   4.56   0.38      8\n",
      "2       3   0.000  2.00  2.725  10.00   0.14      0\n",
      "3       4   4.672  0.87  0.294   3.37   1.91      6\n",
      "4       5   0.813  0.00  6.173   8.02   3.23      6\n",
      "max vocab=  1405\n",
      "after including ifidf features\n",
      "   dishId                                               tags  \\\n",
      "0       1  Ethyl Lactate|3,4-Dihydroxybenzaldehyde|DL-Liq...   \n",
      "1       2  AC1LDI49|56424-87-4|3,4-Dihydroxybenzaldehyde|...   \n",
      "2       3  3-Methyl-1-butanol|Thymol|2-Nonanone|Pyrrolidi...   \n",
      "3       4  AC1LDI49|56424-87-4|2-Hexenyl propanoate|3,4-D...   \n",
      "4       5  3,4-Dihydroxybenzaldehyde|DL-Liquiritigenin|2-...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [ethyl lactate, 3,4-dihydroxybenzaldehyde, dl-...   \n",
      "1  [ac1ldi49, 56424-87-4, 3,4-dihydroxybenzaldehy...   \n",
      "2  [3-methyl-1-butanol, thymol, 2-nonanone, pyrro...   \n",
      "3  [ac1ldi49, 56424-87-4, 2-hexenyl propanoate, 3...   \n",
      "4  [3,4-dihydroxybenzaldehyde, dl-liquiritigenin,...   \n",
      "\n",
      "                                            features  \n",
      "0    (0, 0)\\t0.44264974364151893\\n  (0, 1)\\t0.108...  \n",
      "1    (0, 0)\\t0.44264974364151893\\n  (0, 1)\\t0.108...  \n",
      "2    (0, 107)\\t0.04436104407808569\\n  (0, 108)\\t0...  \n",
      "3    (0, 0)\\t0.44264974364151893\\n  (0, 1)\\t0.108...  \n",
      "4    (0, 3)\\t0.024601495005335164\\n  (0, 9)\\t0.02...  \n",
      "trainig shape=  (55930, 3)\n",
      "testing shape=  (13983, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-229a7b27e631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muserId\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpredict_on\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0moriginal_rating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dishId'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0moriginal_rating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'dishId'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'userId'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rating'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reformed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dishName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fyp\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   7295\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7296\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7297\u001b[1;33m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7298\u001b[0m         )\n\u001b[0;32m   7299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fyp\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fyp\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fyp\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1144\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                 ):\n\u001b[1;32m-> 1146\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted_test_error, predictions = main(data, db, predict_on = predict_on, include_flavours = flavours)\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns = ['dishId', 'rating'])\n",
    "predictions = predictions.merge(dishes, on = 'dishId', how = 'left')\n",
    "predictions.columns = ['dishId', 'rating', 'dishName']\n",
    "\n",
    "if flavours:\n",
    "    if os.path.exists(\"../dataset/Utilities/tfidf_final_flavour_scores.pickle\"):\n",
    "        final_scores = pickle.load(open(\"../dataset/Utilities/tfidf_final_flavour_scores.pickle\", \"rb\" ))\n",
    "        final_scores[predict_on] = predictions\n",
    "    else:\n",
    "        final_scores = {}\n",
    "        final_scores[predict_on] = predictions\n",
    "\n",
    "    pickle.dump(final_scores, open('../dataset/Utilities/tfidf_final_flavour_scores.pickle', 'wb'))\n",
    "\n",
    "else:\n",
    "    if os.path.exists(\"../dataset/Utilities/tfidf_final_scores.pickle\"):\n",
    "        final_scores = pickle.load(open(\"../dataset/Utilities/tfidf_final_scores.pickle\", \"rb\" ))\n",
    "        final_scores[predict_on] = predictions\n",
    "\n",
    "    else:\n",
    "        final_scores = {}\n",
    "        final_scores[predict_on] = predictions\n",
    "\n",
    "    pickle.dump(final_scores, open('../dataset/Utilities/tfidf_final_scores.pickle', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 9974, 'predicted_test_error': 0.49264960426279847, 'time': 1585329269.24, 'predicted_rating':       dishId  rating                      dishName\n",
      "0          1     5.0      curried green bean salad\n",
      "1          2     5.0                    keema aloo\n",
      "2          3     5.0                       paratha\n",
      "3          4     5.0       black chana with potato\n",
      "4          5     5.0     tomato cucumber kachumbar\n",
      "...      ...     ...                           ...\n",
      "1375    1376     5.0              watermelon juice\n",
      "1376    1377     5.0              watermelon juice\n",
      "1377    1378     5.0         white sauce for pasta\n",
      "1378    1379     5.0         white sauce for pasta\n",
      "1379    1380     5.0  white sauce recipe for pasta\n",
      "\n",
      "[1380 rows x 3 columns], 'original_rating':     dishId  userId  rating  reformed              dishName\n",
      "0      998    9974       5       5.0       acai fruit bowl\n",
      "1      998    9974       5       5.0       acai fruit bowl\n",
      "2     1188    9974       5       5.0  broccoli pesto pasta\n",
      "3     1188    9974       5       5.0  broccoli pesto pasta\n",
      "4      203    9974       5       5.0                carrot\n",
      "5      203    9974       5       5.0                carrot\n",
      "6     1332    9974       5       5.0       chicken fajitas\n",
      "7     1332    9974       5       5.0       chicken fajitas\n",
      "8      651    9974       5       5.0    spiced moong beans\n",
      "9      651    9974       5       5.0    spiced moong beans\n",
      "10    1368    9974       5       5.0   spicy chicken tacos\n",
      "11    1368    9974       5       5.0   spicy chicken tacos}\n"
     ]
    }
   ],
   "source": [
    "data = data[data.userId == predict_on]\n",
    "data[\"dishId\"]=data[\"dishId\"].astype(np.int64)\n",
    "original_rating = data.merge(predictions, how = 'left', on = 'dishId')\n",
    "original_rating.columns = ['dishId', 'userId', 'rating', 'reformed', 'dishName']\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "answer = {\"user\" : predict_on, \"predicted_test_error\": predicted_test_error, \"time\" : round(time_end - 0, 2), \"predicted_rating\" : predictions, \"original_rating\" : original_rating}\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fyp] *",
   "language": "python",
   "name": "conda-env-fyp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
